ADR-WCF-001a: Mobile Client Technology Stack Selection Strategy & Comparative Proof-of-Concept Mandate
Status: PROPOSED (Decision Contingent on Rigorous Comparative PoC Outcome)
Date: 2025-06-01 (Revised to detail a comprehensive, comparative PoC process for selecting between React Native and Flutter, incorporating extensive feedback on PoC scope, metrics, human/AI roles, timelines, and environmental consistency)
Authors: WorldChef Development Team
1. Context and Problem Statement
The WorldChef project requires a high-performance, maintainable mobile client for iOS and Android, to be developed with significant AI code generation (target 99.9%) under limited human oversight capacity (20h/week). The application will be media-heavy. The foundational decision of the mobile technology stack (React Native vs. Flutter) carries profound implications. This ADR mandates a rigorous, comparative Proof-of-Concept (PoC) phase to gather empirical evidence before a final stack commitment, ensuring the choice aligns with performance NFRs, AI-assisted development efficiency, and overall project goals.
2. Decision Drivers
Performance for Media-Heavy Application (Critical): Must achieve smooth 60fps scrolling for image-laden lists and fast screen loads (p95 TTI <1.5-2s) on defined mid-range test devices.
AI Code Generation Effectiveness & Human Oversight Efficiency: Chosen stack must enable AI to generate high-quality, idiomatic, performant code, with human review/optimization effort fitting within capacity.
High-Quality, Branded User Experience: Ability to implement Figma designs with fidelity.
Cross-Platform Development Efficiency & Single Codebase.
Development Velocity (MVP): Enable rapid iteration.
Native Feature Access, Offline Capability, Accessibility & Internationalization.
Ecosystem, Testability, Long-Term Maintainability.
(Explicit Assumption: AI agents are primary code generators. Human oversight is for architecture, complex logic, performance tuning, QA, and AI guidance. The 20h/week human capacity is a strict constraint influencing feasibility.)
3. Primary Contenders & PoC Mandate
Primary Contenders:
React Native (with New Architecture & Expo Ecosystem Tools)
Flutter
(Detailed pros/cons of each are well-understood from previous evaluations. This ADR now focuses on the PoC to empirically compare them for WorldChef's specific context.)
Decision:
The final selection of the Mobile Client Technology Stack (React Native or Flutter) is CONTINGENT upon the outcomes of the mandatory, time-boxed, comparative Proof-of-Concept (PoC) development and benchmarking phase detailed below. This ADR will be updated with the final selection and its evidence-based rationale post-PoC.
4. Comparative Proof-of-Concept (PoC) Detailed Plan
4.1. Overall PoC Objective:
To empirically determine which framework (React Native with New Arch vs. Flutter) provides the best balance of (1) achievable performance for WorldChef's media-heavy UI, (2) AI code generation effectiveness coupled with human oversight efficiency, (3) ability to meet core UX and basic non-functional requirements (offline, A11y, i18n stubs), within the project's constraints.
4.2. PoC Timeframe & Resource Allocation:
Total Calendar Timebox: 4 weeks maximum for both PoCs combined (e.g., 2 weeks per stack, or parallel if two human overseers are available, though sequential by the same overseer provides better comparison of DX). A hard deadline for PoC completion and decision is critical.
Effort per Stack PoC: Strict timebox of 20-25 human oversight hours (review, debug, optimize, guide AI, document) PLUS AI generation time. AI generation time itself (wall-clock from prompt to usable code snippet) will also be tracked.
Human PoC Lead(s): Explicitly assign lead(s) responsible for overseeing each PoC, ideally with some prior familiarity (even minor) with the respective stack, or willingness to rapidly learn for evaluation. If one person oversees both, their learning curve for the second stack must be noted.
4.3. PoC Implementation Scope (Identical Features for both RN and Flutter PoCs):
Project Setup & Basic CI:
Initialize project with standard tooling (Expo CLI for RN with New Arch enabled; flutter create for Flutter).
Minimal CI pipeline (ADR-WCF-011) to build PoC and run basic lint/tests.

Media-Heavy Recipe List Screen:
Scrollable list (~50 RecipeCard components).
RecipeCard: Image (async load from mock URL, consistent sizes), title, creator, static like/fav icons.
Best-practice list virtualization (@shopify/flash-list for RN; ListView.builder for Flutter).
Image caching (react-native-fast-image or equivalent for RN; cached_network_image for Flutter).

Recipe Detail Screen (Core Structure):
Hero image, title, text description, simple text list of ingredients/steps.

Basic Navigation: Between List and Detail screens (using standard navigation libraries for each stack).
Simple Shared UI State: Theme toggle (light/dark) affecting background, managed by a simple global store (Zustand snippet for RN; Provider/Riverpod snippet for Flutter).
Basic Offline Caching Stub: Demonstrate caching of recipe list data (mock JSON) locally (AsyncStorage/MMKV for RN; shared_preferences/Hive for Flutter) and rendering from cache on app restart if network is "mock-unavailable." Measure cache write/read time.
Basic Native Module Integration Stub: Implement a simple action triggering a native platform feature (e.g., request camera permission, show a local notification using expo-notifications / flutter_local_notifications).
Basic Accessibility & Internationalization Stubs:
Ensure all tappable elements on one screen have accessibilityLabel / Semantics.label.
Implement one screen with 1-2 hardcoded strings loaded via a basic i18n setup (e.g., i18next for RN; Flutter's intl package).
Test one screen in an RTL language layout (e.g., Arabic/Hebrew device setting).

4.4. AI Interaction Model & Human Oversight Role (Consistent for both PoCs):
AI will be tasked with generating the initial implementation of PoC features based on detailed prompts derived from Figma and feature scope.
Human effort will then focus on:
Reviewing AI-generated code for correctness, idiomatic patterns, and maintainability.
Debugging issues in AI-generated code.
Guiding the AI with further prompts for necessary refactoring or alternative implementations.
Performing/Guiding performance optimizations on the AI-generated codebase.
Implementing parts the AI struggles significantly with, if within the timebox.

Time Tracking Methodology: "AI generation time" (wall-clock from first prompt to usable initial code for a feature part) and "Human intervention hours" (logged in a shared timesheet for review, debug, optimization, AI re-prompting) must be tracked separately for each feature part of the PoC.
4.5. Test Devices & Environment (Consistent for both PoCs):
Physical Devices (Prioritized for Performance):
Android: Google Pixel 4a (or specified equivalent, e.g., ~4-6GB RAM, mid-range CPU of its era).
iOS: iPhone 11 (or specified equivalent).
Explicit OS versions to be used.

Emulators/Simulators: Used for general development, but final performance/UX testing on physical devices.
Network Simulation: Consistent network throttling (e.g., "Fast 3G" profile using device developer options or Charles/Proxyman) for testing image loading and offline behavior. Mock server for images will have consistent (mocked) response times.
AI Tooling: Same AI model/platform (e.g., specific GPT version via API, or same Copilot settings) and consistent high-level prompting strategies (adapted for language/framework specifics) used for both PoCs.
4.6. PoC Evaluation Criteria:
Performance (Quantitative & Qualitative - Primary Gate):
Metric Collection: Use Flipper/RN Perf Monitor & Flutter DevTools (Performance/CPU Profiler, Track Widget Builds). For scrolling: 3 x 30-second scroll segments on Recipe List, record 90th/95th percentile frame build times and rasterizer times.
Target: Consistent ~60fps scrolling on Recipe List (e.g., 95th percentile frame time < 16-17ms).
Target: Screen TTI / perceived load for List & Detail < 1.5 seconds (post-initial app load).
Smoothness of image loading, absence of jank (qualitative, but supported by frame data).
Memory usage (stable, no excessive leaks during PoC tasks).
Final PoC app bundle size (release build).

AI Code Generation & Human Oversight Efficiency (Quantitative & Qualitative):
Total "AI generation time" per stack.
Total "Human intervention hours" per stack.
Subjective assessment (1-5 rubric) of AI's ability to produce idiomatic, maintainable, performant code.
Subjective assessment (1-5 rubric) of ease/difficulty for human to review, debug, and optimize AI code.

Developer Experience (DX) (Human Oversight Perspective - Qualitative, 1-5 Rubric):
Ease of project setup, build times, hot reload/fast refresh effectiveness.
Debugging experience, quality of error messages.
Quality/accessibility of official documentation and community resources for issues encountered.

UI Fidelity & Feature Completeness within PoC Scope (Qualitative, 1-5 Rubric):
Ability to achieve Figma design specs for PoC screens.
Successful implementation of all PoC features (list, detail, nav, state, offline stub, native stub, basic A11y/i18n).

Stability & Robustness (Quantitative & Qualitative):
Number of crashes or critical bugs encountered per stack during PoC.
Subjective assessment of overall PoC app stability.

Basic A11y/i18n Check (Qualitative):
Does VoiceOver/TalkBack read labels correctly? Does RTL layout work without code changes?

4.7. Definition of PoC "Showstopper" / Failure for a Stack:
Consistent failure to meet critical performance targets (e.g., list scrolling average frame time > 25-30ms, TTI > 3s) despite dedicated, time-boxed optimization effort (e.g., 5-8 hours of the human oversight budget per stack focused on performance).
AI code generation proves unmanageable, requiring >75% human rewrite for core PoC functionality.
Fundamental roadblocks with core libraries or framework capabilities for the PoC scope that cannot be resolved within the PoC timebox.
Inability to complete the defined PoC scope within the overall PoC timebox for that stack.
5. Rationale for PoC-Driven Approach
Given the critical impact of the mobile stack choice, the project's unique AI-driven development model, and the demanding nature of a media-heavy application, a PoC-driven comparative evaluation is essential. It replaces assumptions with empirical data specific to WorldChef's context, enabling an evidence-based final decision. This mitigates the high risk of committing to a suboptimal stack based on theoretical advantages or incomplete information.
6. Consequences (of adopting this PoC-driven strategy)
Positive Consequences / Benefits:
Evidence-Based Decision: Final stack choice will be backed by concrete data from WorldChef-specific PoCs.
Risk Reduction: Significantly de-risks performance, AI development efficiency, and DX for the chosen stack.
Increased Team Confidence: Team will have hands-on experience and data to support the final choice.
Early Identification of Stack-Specific Challenges: Uncovers nuanced issues with either stack early.
Negative Consequences / Trade-offs / Risks:
Upfront Time Investment: Dedicating 4 weeks (and 40-50 human hours + AI time) to PoCs delays the start of full MVP feature development on the final stack. This is a deliberate investment in de-risking.
PoC Scope Creep: Risk of PoCs becoming too large. (Mitigation: Strict adherence to defined PoC scope).
Bias in Evaluation: Requires structured, objective evaluation against defined criteria to minimize human bias. (Mitigation: Pre-defined rubrics, consistent metrics collection).
Contingency if Both PoCs Show Major Issues: If neither stack performs adequately or is manageable with AI, a more fundamental project re-evaluation (scope, resources, or even core tech assumptions) would be needed. This ADR assumes at least one will be viable.
7. Validation / Success Metrics (for this ADR / PoC Phase Itself)
Both RN and Flutter PoCs are completed within the defined timebox and scope.
Clear, comparable quantitative and qualitative data is gathered for all PoC evaluation criteria.
A definitive, evidence-based decision for the mobile client technology stack is made, documented in an updated version of this ADR, and communicated with clear rationale.
The project proceeds with high confidence in the chosen stack's ability to meet MVP requirements.
8. Review / Revisit
This ADR will be updated with the final stack choice and detailed rationale based on PoC outcomes. The PoC results and evaluation report will become key appendices or referenced documents.
If PoCs results are ambiguous or both stacks present significant showstoppers, the core assumptions of the project (e.g., feature scope vs. capacity, AI effectiveness) may need revisiting before a final stack decision can be made.



